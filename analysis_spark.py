# -*- coding: utf-8 -*-
"""Analysis_spark

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zrwgMBIH2B_WcwZTLK6TH30RHj__1Q2_
"""

!pip3 install pyspark
from pyspark.sql import SparkSession
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType,ArrayType
import math 
import pyspark.sql.functions as f
from pyspark.sql import types 
from pyspark.sql import Window
spark = SparkSession.builder.master("local[*]") \
                    .appName('Analysis') \
                    .getOrCreate()

"""Summary Report (All Time)"""

df=spark.read.csv("/content/datafiles",header=True)
meta=spark.read.csv("/content/meta/symbol_metadata.csv",header=True)
df1=df.withColumn("filename",f.input_file_name())
df2=df1.withColumn("filename",f.regexp_replace(f.split(df1['filename'], '/').getItem(5),'.csv',''))
df3=df2.join(meta,f.trim(df2.filename)==f.trim(meta.Symbol),"inner")
df3.orderBy('timestamp') \
    .groupBy("Sector") \
    .agg(f.round(f.avg('open'),2).alias('Avg_OpenPrice'), \
         f.round(f.avg('close'),2).alias('Avg_ClosePrice'), \
         f.round(f.max('high'),2).alias('Max_high'), \
         f.round(f.min('low'),2).alias('Min_low'), \
         f.round(f.avg('volume'),2).alias('Avg_volume')).show()

"""Aggregation summary details for a given sector(s) over the time period."""

#------------------input---------------------------------
dt=[('2021-01-01','2022-05-26',['TECHNOLOGY','FINANCE'])]
#----------------------------------------------------------
deptSchema = StructType([       
    StructField('dept_name', StringType(), True),
    StructField('dept_id', StringType(), True),
    StructField('array_valuer',ArrayType(StringType(),False)  ,True)
])
unioned_df = None
df_filter  =spark.createDataFrame(data=dt,schema=deptSchema)
df1_filter =df_filter.withColumn("sector_value",f.explode(f.col('array_valuer'))).drop('array_valuer')
dd_c={x[2]:[x[0],x[1]] for x in df1_filter.rdd.collect()}  
for i,k in dd_c.items():
  sector_value= i
  start_date=k[0]
  end_date  =k[1]
  df_temp   =df3.filter((f.col("timestamp")>=start_date) & (f.col("timestamp")<=end_date)&(f.col("Sector")==sector_value))  
  
  if not unioned_df:
        unioned_df = df_temp
  else:
        unioned_df = unioned_df.union(df_temp)
agg_df=unioned_df.orderBy('timestamp') \
          .groupBy("Sector") \
          .agg(f.round(f.avg('open'),2).alias('Avg_OpenPrice'), \
           f.round(f.avg('close'),2).alias('Avg_ClosePrice'), \
           f.round(f.max('high'),2).alias('Max_high'), \
           f.round(f.min('low'),2).alias('Min_low'), \
           f.round(f.avg('volume'),2).alias('Avg_volume'))

agg_df.show()

"""Aggregation summary details for all the symbol(s) in a given sector over
the time period.
"""

#------------------input---------------------------------
dt=[('2021-01-01','2022-05-26',['TECHNOLOGY','FINANCE'])]
#----------------------------------------------------------
deptSchema = StructType([       
    StructField('dept_name', StringType(), True),
    StructField('dept_id', StringType(), True),
    StructField('array_valuer',ArrayType(StringType(),False)  ,True)
])
unioned_df1 = None
df_filter1  =spark.createDataFrame(data=dt,schema=deptSchema)
df1_filter1 =df_filter.withColumn("sector_value",f.explode(f.col('array_valuer'))).drop('array_valuer')
dd_c1={x[2]:[x[0],x[1]] for x in df1_filter.rdd.collect()}  
for i,k in dd_c1.items():
  sector_value= i
  start_date=k[0]
  end_date  =k[1]
  df_temp   =df3.filter((f.col("timestamp")>=start_date) & (f.col("timestamp")<=end_date)&(f.col("Sector")==sector_value))  
  if not unioned_df:
        unioned_df = df_temp
  else:
        unioned_df = unioned_df.union(df_temp)
agg_df1=unioned_df.orderBy('timestamp') \
          .groupBy("Symbol","Name") \
          .agg(f.round(f.avg('open'),2).alias('Avg_OpenPrice'), \
           f.round(f.avg('close'),2).alias('Avg_ClosePrice'), \
           f.round(f.max('high'),2).alias('Max_high'), \
           f.round(f.min('low'),2).alias('Min_low'), \
           f.round(f.avg('volume'),2).alias('Avg_volume'))
agg_df1.show()